<title> Ingress k8s - Let's play around it </title>
<center> Ingress k8s - Let's play around it </center>
<br/>
<hr/>
<br/>
Hey all, I am going to talk about Ingress in Kubernetes, and for educational purposes we gonna tweak it so we can understand all the common behaviors of Ingress.<br/>
Let's first give a brief description about Ingress, in order to expose your application(s) which are running as service(s) that are running in k8s pods to outside the cluster.<br/>
This applications/services are going to be used by people which obviously they are living outside of your k8s cluster, so we need to define an Ingress resource where it specifies <br/>
how we want external clients to connect to our services. <br/>

Here is a simple snippet of YAML code to define an Ingress object.<br/>
<pre>
kind: Ingress
apiVersion: extensions/v1beta1
metadata:
  name: k8s-ingress
spec:
 rules:
   - host: k8s.nzoueidi.com
     http:
       paths:
       - backend:
           serviceName: k8s-nzoueidi
           servicePort: 80
</pre> 
Here for example the Ingress controller configures itself so it can reads the Ingress resource and routes the incoming traffic.<br/>
It would act as <b>L7 LB</b>. An <b>L7 LB</b> is <b>"Layer 7 Load Balancing"</b>, it operates at the high-level application layer, which deals with the actual content of each message. <br/>
L7 LB terminates the network traffic and reads the messange whithin. And the most important part is that it makes a load-balancing decision based on the content of the message<br/>
that went through the network.. It then make a new TCP connection to the selected upstream server or in some cases resues an existing one which is applicable to HTTP keepalives. <br/>
Anyway, this is not our topic, you can duckduck it and you will have many examples of L7 load-balancing.. <br/>
Let's back to Ingress, many people expose their apps using <i>Port Forwarding</i>, this is not good for production matter as it simply forwards the requests you get from port on the client machine<br/>
directly to the targeted pod. Also, in the same context we have "HostPort", in some places they call it as <i>Host Network</i>, it is a mechanism where ports on a worker node in our k8s cluster <br/>
are directly mapped to ports on pods, which is silly - here you need to have these ports opened for outside communcation and as a limitation, here you need to have only one pod per node.. <br/>
Before, we go through many good serious ways, we want to know why the hell we want to use Ingress in any similar scenarios. Alright, let's describe the low level of our network stack. <br/>
In a general way, connections and requests theorically are operating through Layer 4 which is TCP or Layer 7 which is http, rpc, etc.. In the kernel side, <b>Netfilter</b> is the one which is responsible<br/>
for routing rules, this is basically in the Layer 3 of <i>OSI</i> which is the Network Layer. If we gonna talk about Netfilter, this article will be long and bored. So briefly <b>Netfilter</b> have five principale<br/>
hooks that programs can register with. As packets are going through the stack, they will trigger some kernel modules that have registred with these hooks, it depends on the packet itself, if it is <br/>
icoming or outgoing, also the packet desitination and wether the packet was dropred or rejected. Anyway..a packet when it is coming to our cluster, more specifically to our node's desired interface<br/>
is processed by <b>Netfilter</b>, matches the rules established and it is forwarded to the IP of the available and healthy pod.<br/>
So, to expose your applications we need to implement the same overview above. Thus, our end-users will have to call the cluster IP and port because this is where all the toolchain described starts. <br/>
But, the IP we are talking about is only reachable from inside the cluster. The question here how the heck we can forward a public IP endpoint to and IP that is only reachable once the packet<br/>
is already on a node?<br/>
You can use many <b>"Tell"</b> ways:
<ul>
	<li> Examine the netfilter rules and make a use from iptables's utility, the issue here that node(s) are <i>ephemeral</i>, same thing for pod(s) <br/>
		and also the big issue here that rules you made are operating at Layer 3 (we talked about above) they don't know if your services are healthy or not.<br/>
	If the node is somehow unreachable the route also also will remain unreachable and stay broken.</li>
	<li> Put everything in the k8s master, well I am sure that you would not do that as long as you are a human being. </li>
	<li> Use a Simple LoadBalancer in front for distributing client traffic to the nodes in our cluster, for this we need a Public IP address and the addresses of the nodes.<br/>
	The issue here that when an end-user wnat to connect to our service using other port (e.g 80) we can not afford that the request of that packets using that port to the node's interface..<br/>
        This is because we don't have any kind of listners like processes on the IP address of the targeted node. </li>
	<li> There are other many non-good ways..</li>
</ul>
<br/>
To prevent these issues, Kubernetes solve it by something called <b><i>NodePort</i></b>. Here is a snippet for <b>NodePort</b> service in k8s. <br/>
<pre>
kind: Service
apiVersion: v1
metadata:
  name: service-nodeport
  spec:
    type: NodePort
      selector:
          app: nodeport-app
	    ports:
	      - port: 80
	      targetPort: http
</pre>
A service like this is a <b>ClusterIP</b> service with the a nice capability which is: the IP of the node is reachable as well as the cluster IP. How it works exactly?<br/>
It tells <i>kube-proxy</i> to allocate a port in a regular range and opens the associated port in every node's interface. Like this we can make sure that any traffic can be routed to any node without any problems.<br/>
So, here we used NodePort to expose our service to our end-users on a "fake" port. This is good while our load balancer can reach the real port internally and hide it from the end-user. <br/>
But, this is not suffice as a complete a solution for loadbalancing our service(s). This is where Ingress shows up. <br/>
Another service of type <b>LoadBalancer</b>, it contains the capababilities of <i>NodePort</i> service and the ability to have a complete Ingress path. <br/>
If you are using a cloud provider which have or supports API driven configuration of networking resources, you can try this snippet of YAML code. <br/>
<pre>
kind: Service
apiVersion: v1
metadata:
  name: service-loadbalancer
  spec:
    type: LoadBalancer
      selector:
          app: loadbalancer-app
	    ports:
	      - port: 80
	      targetPort: http
</pre>
You will notice that an external IP has been allocated by typing <pre> kubectl get svc service-loadbalancer </pre>
To be fair, the service <b>LoadBalancer</b> have some limitations like it can not determine HTTPS traffic in the Loadbalancer level. <br/>
Back, to Ingress we have 3 main componenets which are the following:<br/>
- <b>Ingress Resource:</b> is a k8s resource which defines rules that are used by the Ingress Controller to route only incoming traffic. <br/>
- <b>Ingress Controller:</b> an application which captures incoming requests and routes them accordinly by the rules that are already mentioned in the Ingress Resource. <br/>
- <b>Default Backend:</b> it is simply a small application that catches all the traffic that are not relevant rules defined by the Ingress Resource and shows up a 404 page. <br/>
<br/><br/>
 
Our goal here as mentioned in the first of the article is playing around with Ingress so we can tweak it. <br/>
Let's give a good scenario where we can see the importance of why we should know really how Ingress works and more than deep dive into it. <br/>
Let's say we have a kubernetes cluster, where we have an application that is being proxied through nginx. For some weird reasons our app started taking too long to respond<br/>
Thus, caused connections and requests in general to fell completly into <b>nginx listen backlog</b> what made nginx itself to start drop connections including the most important ones that were being made<br/>
by kubernetes.. <br/>
An <b> nginx listen backlog</b> is a place where all the pending connections are stored for further usage. It is actually a parameter invoked into the listen() syscall. <br/>
The connections that are being made by kubernetes are actually "liveness and readiness probes for containers", <b>kubelet</b> uses it to have the knowledge when exactly to restart a container in a regular pod. <br/>
If somehow, the pod fails to respond to the liveness proves we talked about, k8s thinks here that there is absolutely something wrong and it restarts the pod, while our issue really is not about liveness probes.. <br/>
This is also can lead to a crash-loop after several restarts.. <br/>
This is an image where it gives an overview of the TCP backlog in Linux where it is implemened due the Listen syscall. <br/>
<br/>
<center><img src="https://camo.githubusercontent.com/c3da9d0cb01dae917379a15cd475af3134fe0ba1/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f612f61322f5463705f73746174655f6469616772616d5f66697865642e737667"></center> 
<br/>
We can go deeper about this to the TCP backlog and how it works under the hoods in Linux, but this could be another separate article :) <br/>
<br/>
Another article will follow on how tweak code and packages into the <a href="https://github.com/kubernetes/ingress-nginx">Ingress-nginx</a> repository


Cheers <b>o/</b>
<i>Last modified on 09/11/2018 5:28 PM</i>


